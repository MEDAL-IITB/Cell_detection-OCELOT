{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba8ff411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import glob\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import segmentation_models_pytorch as smp\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import jaccard_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c29ddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = glob.glob('/workspace/jay/DDP/Ocelot/yolo_binary/datasets/cell_detect_33-1/valid/images/*.jpg')\n",
    "test = glob.glob('/workspace/jay/DDP/Ocelot/yolo_binary/datasets/cell_detect_33-1/test/images/*.jpg')\n",
    "val_files = np.unique(np.array([x.split('/')[-1][:3] for x in val]))\n",
    "test_files = np.unique(np.array([x.split('/')[-1][:3] for x in test]))\n",
    "val_set = list(val_files)+list(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59e50b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ocelot(Dataset):\n",
    "    def __init__(self,val=False):\n",
    "        self.val = val\n",
    "        temp = glob.glob('/workspace/jay/DDP/Ocelot/ocelot2023/images/train/cell/*.jpg')\n",
    "        self.cell_imgs = [x for x in temp if x.split('/')[-1][:3] not in val_set]\n",
    "        self.val_imgs = [x for x in temp if x.split('/')[-1][:3] in val_set]\n",
    "\n",
    "        self.masks = ['/workspace/jay/DDP/Ocelot/ocelot2023/cell_seg_masks/'+x.split('/')[-1][:3]+'.npy' for x in self.cell_imgs]\n",
    "        self.val_masks = [f'/workspace/jay/DDP/Ocelot/ocelot2023/cell_seg_masks/'+x.split('/')[-1][:3]+'.npy' for x in self.val_imgs]                 \n",
    "        \n",
    "    def __len__(self): \n",
    "        if self.val:\n",
    "            return len(self.val_imgs)\n",
    "        else:\n",
    "            return len(self.cell_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.val:\n",
    "            file = self.val_imgs[idx]\n",
    "            mask = self.val_masks[idx]\n",
    "        else:\n",
    "            file = self.cell_imgs[idx]\n",
    "            mask = self.masks[idx]\n",
    "            \n",
    "        name = file.split('/')[-1].split('.')[0]\n",
    "        cell = Image.open(file)\n",
    "        mask = torch.Tensor(np.load(mask))\n",
    "        \n",
    "        if not self.val:\n",
    "            if random.uniform(0, 1)>0.5:\n",
    "                cell = T.functional.hflip(cell)\n",
    "                mask = T.functional.hflip(mask)\n",
    "            if random.uniform(0, 1)>0.5:\n",
    "                rot = random.uniform(0,1)\n",
    "                if rot<0.5 and rot>=0.25:\n",
    "                    cell = T.functional.rotate(cell,angle=90)\n",
    "                    mask = T.functional.rotate(mask,angle=90)\n",
    "                elif rot>=0.5 and rot<0.75:\n",
    "                    cell = T.functional.rotate(cell,angle=180)\n",
    "                    mask = T.functional.rotate(mask,angle=180)\n",
    "                elif rot>=0.75 and rot<=1:\n",
    "                    cell = T.functional.rotate(cell,angle=270)\n",
    "                    mask = T.functional.rotate(mask,angle=270)\n",
    "            if random.uniform(0, 1)>0.5:\n",
    "                cell = T.functional.adjust_brightness(cell, 0.1)\n",
    "                cell = T.functional.adjust_contrast(cell,0.1)\n",
    "                cell = T.functional.adjust_saturation(cell,0.1)\n",
    "                cell = T.functional.adjust_hue(cell,0.1)\n",
    "\n",
    "        cell = np.array(cell)\n",
    "        cell = cell / 255\n",
    "        cell = cell - 0.5\n",
    "        cell = torch.Tensor(np.moveaxis(cell, -1, 0))\n",
    "    \n",
    "        return cell,mask,name\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97ffdcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:2'\n",
    "\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights='imagenet',     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=3,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "# model = smp.DeepLabV3Plus(\n",
    "#     encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "#     encoder_depth=5,\n",
    "#     encoder_weights='imagenet',     # use `imagenet` pre-trained weights for encoder initialization\n",
    "#     activation=None,\n",
    "#     in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "#     classes=3,                      # model output channels (number of classes in your dataset)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eec269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "model = model.to(device)\n",
    "ds = Ocelot(val=False)\n",
    "dl = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "val_ds = Ocelot(val=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "criterion = smp.losses.DiceLoss(smp.losses.MULTILABEL_MODE, from_logits=True)\n",
    "#criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "#criterion = smp.losses.FocalLoss(mode=smp.losses.MULTILABEL_MODE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=5e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=5e-5)\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e027b546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7aea12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Train loss = 0.6800\n",
      "0: Validation loss = 0.5197\n",
      "Best val loss updated, model saved\n",
      "------------------------------------------------\n",
      "1: Train loss = 0.6236\n",
      "1: Validation loss = 0.4560\n",
      "Best val loss updated, model saved\n",
      "------------------------------------------------\n",
      "2: Train loss = 0.5976\n",
      "2: Validation loss = 0.5544\n",
      "------------------------------------------------\n",
      "3: Train loss = 0.5512\n",
      "3: Validation loss = 0.3945\n",
      "Best val loss updated, model saved\n",
      "------------------------------------------------\n",
      "4: Train loss = 0.5064\n",
      "4: Validation loss = 0.3853\n",
      "Best val loss updated, model saved\n",
      "------------------------------------------------\n",
      "5: Train loss = 0.4788\n",
      "5: Validation loss = 0.3831\n",
      "Best val loss updated, model saved\n",
      "------------------------------------------------\n",
      "6: Train loss = 0.4611\n",
      "6: Validation loss = 0.3537\n",
      "Best val loss updated, model saved\n",
      "------------------------------------------------\n",
      "7: Train loss = 0.4496\n",
      "7: Validation loss = 0.3638\n",
      "------------------------------------------------\n",
      "8: Train loss = 0.4373\n",
      "8: Validation loss = 0.3637\n",
      "------------------------------------------------\n",
      "9: Train loss = 0.4435\n",
      "9: Validation loss = 0.3433\n",
      "Best val loss updated, model saved\n",
      "------------------------------------------------\n",
      "10: Train loss = 0.4323\n",
      "10: Validation loss = 0.3388\n",
      "Best val loss updated, model saved\n",
      "------------------------------------------------\n",
      "11: Train loss = 0.4244\n",
      "11: Validation loss = 0.3351\n",
      "Best val loss updated, model saved\n",
      "------------------------------------------------\n",
      "12: Train loss = 0.4211\n",
      "12: Validation loss = 0.3470\n",
      "------------------------------------------------\n",
      "13: Train loss = 0.3997\n",
      "13: Validation loss = 0.3279\n",
      "Best val loss updated, model saved\n",
      "------------------------------------------------\n",
      "14: Train loss = 0.4121\n",
      "14: Validation loss = 0.3232\n",
      "Best val loss updated, model saved\n",
      "------------------------------------------------\n",
      "15: Train loss = 0.4103\n",
      "15: Validation loss = 0.3309\n",
      "------------------------------------------------\n",
      "16: Train loss = 0.4080\n",
      "16: Validation loss = 0.3304\n",
      "------------------------------------------------\n",
      "17: Train loss = 0.3980\n",
      "17: Validation loss = 0.3434\n",
      "------------------------------------------------\n",
      "18: Train loss = 0.3968\n",
      "18: Validation loss = 0.3265\n",
      "------------------------------------------------\n",
      "19: Train loss = 0.4068\n",
      "19: Validation loss = 0.3205\n",
      "Best val loss updated, model saved\n",
      "------------------------------------------------\n",
      "20: Train loss = 0.3811\n",
      "20: Validation loss = 0.3326\n",
      "------------------------------------------------\n",
      "21: Train loss = 0.3857\n",
      "21: Validation loss = 0.3253\n",
      "------------------------------------------------\n",
      "22: Train loss = 0.4013\n",
      "22: Validation loss = 0.3248\n",
      "------------------------------------------------\n",
      "23: Train loss = 0.3762\n",
      "23: Validation loss = 0.3166\n",
      "Best val loss updated, model saved\n",
      "------------------------------------------------\n",
      "24: Train loss = 0.3708\n",
      "24: Validation loss = 0.3101\n",
      "Best val loss updated, model saved\n",
      "------------------------------------------------\n",
      "25: Train loss = 0.3822\n",
      "25: Validation loss = 0.3174\n",
      "------------------------------------------------\n",
      "26: Train loss = 0.3890\n",
      "26: Validation loss = 0.3075\n",
      "Best val loss updated, model saved\n",
      "------------------------------------------------\n",
      "27: Train loss = 0.3827\n",
      "27: Validation loss = 0.3085\n",
      "------------------------------------------------\n",
      "28: Train loss = 0.3650\n",
      "28: Validation loss = 0.3140\n",
      "------------------------------------------------\n",
      "29: Train loss = 0.3739\n",
      "29: Validation loss = 0.3115\n",
      "------------------------------------------------\n",
      "30: Train loss = 0.3753\n",
      "30: Validation loss = 0.3051\n",
      "Best val loss updated, model saved\n",
      "------------------------------------------------\n",
      "31: Train loss = 0.3630\n",
      "31: Validation loss = 0.3072\n",
      "------------------------------------------------\n",
      "32: Train loss = 0.3669\n",
      "32: Validation loss = 0.3015\n",
      "Best val loss updated, model saved\n",
      "------------------------------------------------\n",
      "33: Train loss = 0.3574\n",
      "33: Validation loss = 0.3035\n",
      "------------------------------------------------\n",
      "34: Train loss = 0.3594\n",
      "34: Validation loss = 0.3042\n",
      "------------------------------------------------\n",
      "35: Train loss = 0.3685\n",
      "35: Validation loss = 0.3091\n",
      "------------------------------------------------\n",
      "36: Train loss = 0.3525\n",
      "36: Validation loss = 0.3134\n",
      "------------------------------------------------\n",
      "37: Train loss = 0.3554\n",
      "37: Validation loss = 0.3169\n",
      "------------------------------------------------\n",
      "38: Train loss = 0.3500\n",
      "38: Validation loss = 0.2967\n",
      "Best val loss updated, model saved\n",
      "------------------------------------------------\n",
      "39: Train loss = 0.3549\n",
      "39: Validation loss = 0.3031\n",
      "------------------------------------------------\n",
      "40: Train loss = 0.3517\n",
      "40: Validation loss = 0.3008\n",
      "------------------------------------------------\n",
      "41: Train loss = 0.3496\n",
      "41: Validation loss = 0.2984\n",
      "------------------------------------------------\n",
      "42: Train loss = 0.3390\n",
      "42: Validation loss = 0.2993\n",
      "------------------------------------------------\n",
      "43: Train loss = 0.3437\n",
      "43: Validation loss = 0.2967\n",
      "------------------------------------------------\n",
      "44: Train loss = 0.3300\n",
      "44: Validation loss = 0.3002\n",
      "------------------------------------------------\n",
      "45: Train loss = 0.3375\n",
      "45: Validation loss = 0.2968\n",
      "------------------------------------------------\n",
      "46: Train loss = 0.3411\n",
      "46: Validation loss = 0.2991\n",
      "------------------------------------------------\n",
      "47: Train loss = 0.3245\n",
      "47: Validation loss = 0.2978\n",
      "------------------------------------------------\n",
      "48: Train loss = 0.3395\n",
      "48: Validation loss = 0.2995\n",
      "------------------------------------------------\n",
      "49: Train loss = 0.3391\n",
      "49: Validation loss = 0.2988\n",
      "------------------------------------------------\n",
      "50: Train loss = 0.3343\n",
      "50: Validation loss = 0.2904\n",
      "Best val loss updated, model saved\n",
      "------------------------------------------------\n",
      "51: Train loss = 0.3298\n",
      "51: Validation loss = 0.2938\n",
      "------------------------------------------------\n",
      "52: Train loss = 0.3317\n",
      "52: Validation loss = 0.2943\n",
      "------------------------------------------------\n",
      "53: Train loss = 0.3283\n",
      "53: Validation loss = 0.2938\n",
      "------------------------------------------------\n",
      "54: Train loss = 0.3216\n",
      "54: Validation loss = 0.3000\n",
      "------------------------------------------------\n",
      "55: Train loss = 0.3293\n",
      "55: Validation loss = 0.3019\n",
      "------------------------------------------------\n",
      "56: Train loss = 0.3322\n",
      "56: Validation loss = 0.2922\n",
      "------------------------------------------------\n",
      "57: Train loss = 0.3301\n",
      "57: Validation loss = 0.2975\n",
      "------------------------------------------------\n",
      "58: Train loss = 0.3382\n",
      "58: Validation loss = 0.3093\n",
      "------------------------------------------------\n",
      "59: Train loss = 0.3279\n",
      "59: Validation loss = 0.3012\n",
      "------------------------------------------------\n",
      "60: Train loss = 0.3282\n",
      "60: Validation loss = 0.3047\n",
      "------------------------------------------------\n",
      "61: Train loss = 0.3358\n",
      "61: Validation loss = 0.3160\n",
      "------------------------------------------------\n",
      "62: Train loss = 0.3347\n",
      "62: Validation loss = 0.2990\n",
      "------------------------------------------------\n",
      "63: Train loss = 0.3475\n",
      "63: Validation loss = 0.3014\n",
      "------------------------------------------------\n",
      "64: Train loss = 0.3321\n",
      "64: Validation loss = 0.2929\n",
      "------------------------------------------------\n",
      "65: Train loss = 0.3525\n",
      "65: Validation loss = 0.3028\n",
      "------------------------------------------------\n",
      "66: Train loss = 0.3382\n",
      "66: Validation loss = 0.2994\n",
      "------------------------------------------------\n",
      "67: Train loss = 0.3396\n",
      "67: Validation loss = 0.3057\n",
      "------------------------------------------------\n",
      "68: Train loss = 0.3371\n",
      "68: Validation loss = 0.3037\n",
      "------------------------------------------------\n",
      "69: Train loss = 0.3349\n",
      "69: Validation loss = 0.3058\n",
      "------------------------------------------------\n",
      "70: Train loss = 0.3339\n",
      "70: Validation loss = 0.3153\n",
      "------------------------------------------------\n",
      "71: Train loss = 0.3528\n",
      "71: Validation loss = 0.2913\n",
      "------------------------------------------------\n",
      "72: Train loss = 0.3377\n",
      "72: Validation loss = 0.3007\n",
      "------------------------------------------------\n",
      "73: Train loss = 0.3458\n",
      "73: Validation loss = 0.3436\n",
      "------------------------------------------------\n",
      "74: Train loss = 0.3419\n",
      "74: Validation loss = 0.3018\n",
      "------------------------------------------------\n",
      "75: Train loss = 0.3335\n",
      "75: Validation loss = 0.3053\n",
      "------------------------------------------------\n",
      "76: Train loss = 0.3532\n",
      "76: Validation loss = 0.3035\n",
      "------------------------------------------------\n",
      "77: Train loss = 0.3460\n",
      "77: Validation loss = 0.3054\n",
      "------------------------------------------------\n",
      "78: Train loss = 0.3469\n",
      "78: Validation loss = 0.3183\n",
      "------------------------------------------------\n",
      "79: Train loss = 0.3549\n",
      "79: Validation loss = 0.3004\n",
      "------------------------------------------------\n",
      "80: Train loss = 0.3442\n",
      "80: Validation loss = 0.3027\n",
      "------------------------------------------------\n",
      "81: Train loss = 0.3634\n",
      "81: Validation loss = 0.3233\n",
      "------------------------------------------------\n",
      "82: Train loss = 0.3544\n",
      "82: Validation loss = 0.3098\n",
      "------------------------------------------------\n",
      "83: Train loss = 0.3620\n",
      "83: Validation loss = 0.3874\n",
      "------------------------------------------------\n",
      "84: Train loss = 0.3630\n",
      "84: Validation loss = 0.2957\n",
      "------------------------------------------------\n",
      "85: Train loss = 0.3465\n",
      "85: Validation loss = 0.3290\n",
      "------------------------------------------------\n",
      "86: Train loss = 0.3500\n",
      "86: Validation loss = 0.2962\n",
      "------------------------------------------------\n",
      "87: Train loss = 0.3410\n",
      "87: Validation loss = 0.3027\n",
      "------------------------------------------------\n",
      "88: Train loss = 0.3600\n",
      "88: Validation loss = 0.3057\n",
      "------------------------------------------------\n",
      "89: Train loss = 0.3513\n",
      "89: Validation loss = 0.3044\n",
      "------------------------------------------------\n",
      "90: Train loss = 0.3467\n",
      "90: Validation loss = 0.3013\n",
      "------------------------------------------------\n",
      "91: Train loss = 0.3571\n",
      "91: Validation loss = 0.3153\n",
      "------------------------------------------------\n",
      "92: Train loss = 0.3631\n",
      "92: Validation loss = 0.3144\n",
      "------------------------------------------------\n",
      "93: Train loss = 0.3605\n",
      "93: Validation loss = 0.3096\n",
      "------------------------------------------------\n",
      "94: Train loss = 0.3481\n",
      "94: Validation loss = 0.3092\n",
      "------------------------------------------------\n",
      "95: Train loss = 0.3522\n",
      "95: Validation loss = 0.3127\n",
      "------------------------------------------------\n",
      "96: Train loss = 0.3390\n",
      "96: Validation loss = 0.3066\n",
      "------------------------------------------------\n",
      "97: Train loss = 0.3520\n",
      "97: Validation loss = 0.3234\n",
      "------------------------------------------------\n",
      "98: Train loss = 0.3334\n",
      "98: Validation loss = 0.3045\n",
      "------------------------------------------------\n",
      "99: Train loss = 0.3455\n",
      "99: Validation loss = 0.2981\n",
      "------------------------------------------------\n",
      "100: Train loss = 0.3415\n",
      "100: Validation loss = 0.3084\n",
      "------------------------------------------------\n",
      "101: Train loss = 0.3601\n",
      "101: Validation loss = 0.3086\n",
      "------------------------------------------------\n",
      "102: Train loss = 0.3572\n",
      "102: Validation loss = 0.2996\n",
      "------------------------------------------------\n",
      "103: Train loss = 0.3384\n",
      "103: Validation loss = 0.3075\n",
      "------------------------------------------------\n",
      "104: Train loss = 0.3480\n",
      "104: Validation loss = 0.2971\n",
      "------------------------------------------------\n",
      "105: Train loss = 0.3343\n",
      "105: Validation loss = 0.3184\n",
      "------------------------------------------------\n",
      "106: Train loss = 0.3349\n",
      "106: Validation loss = 0.3072\n",
      "------------------------------------------------\n",
      "107: Train loss = 0.3438\n",
      "107: Validation loss = 0.3128\n",
      "------------------------------------------------\n",
      "108: Train loss = 0.3615\n",
      "108: Validation loss = 0.3041\n",
      "------------------------------------------------\n",
      "109: Train loss = 0.3524\n",
      "109: Validation loss = 0.3074\n",
      "------------------------------------------------\n",
      "110: Train loss = 0.3283\n",
      "110: Validation loss = 0.3173\n",
      "------------------------------------------------\n",
      "111: Train loss = 0.3419\n",
      "111: Validation loss = 0.2951\n",
      "------------------------------------------------\n",
      "112: Train loss = 0.3303\n",
      "112: Validation loss = 0.3018\n",
      "------------------------------------------------\n",
      "113: Train loss = 0.3248\n",
      "113: Validation loss = 0.2973\n",
      "------------------------------------------------\n",
      "114: Train loss = 0.3380\n",
      "114: Validation loss = 0.3106\n",
      "------------------------------------------------\n",
      "115: Train loss = 0.3394\n",
      "115: Validation loss = 0.3062\n",
      "------------------------------------------------\n",
      "116: Train loss = 0.3292\n",
      "116: Validation loss = 0.2960\n",
      "------------------------------------------------\n",
      "117: Train loss = 0.3286\n",
      "117: Validation loss = 0.2933\n",
      "------------------------------------------------\n",
      "118: Train loss = 0.3332\n",
      "118: Validation loss = 0.3098\n",
      "------------------------------------------------\n",
      "119: Train loss = 0.3331\n",
      "119: Validation loss = 0.3018\n",
      "------------------------------------------------\n",
      "120: Train loss = 0.3289\n",
      "120: Validation loss = 0.3114\n",
      "------------------------------------------------\n",
      "121: Train loss = 0.3393\n",
      "121: Validation loss = 0.2962\n",
      "------------------------------------------------\n",
      "122: Train loss = 0.3274\n",
      "122: Validation loss = 0.3001\n",
      "------------------------------------------------\n",
      "123: Train loss = 0.3230\n",
      "123: Validation loss = 0.3011\n",
      "------------------------------------------------\n",
      "124: Train loss = 0.3194\n",
      "124: Validation loss = 0.2965\n",
      "------------------------------------------------\n",
      "125: Train loss = 0.3275\n",
      "125: Validation loss = 0.3019\n",
      "------------------------------------------------\n",
      "126: Train loss = 0.3240\n",
      "126: Validation loss = 0.3017\n",
      "------------------------------------------------\n",
      "127: Train loss = 0.3182\n",
      "127: Validation loss = 0.3015\n",
      "------------------------------------------------\n",
      "128: Train loss = 0.3080\n",
      "128: Validation loss = 0.3085\n",
      "------------------------------------------------\n",
      "129: Train loss = 0.3132\n",
      "129: Validation loss = 0.2962\n",
      "------------------------------------------------\n",
      "130: Train loss = 0.3241\n",
      "130: Validation loss = 0.2932\n",
      "------------------------------------------------\n",
      "131: Train loss = 0.3081\n",
      "131: Validation loss = 0.2958\n",
      "------------------------------------------------\n",
      "132: Train loss = 0.3083\n",
      "132: Validation loss = 0.2860\n",
      "Best val loss updated, model saved\n",
      "------------------------------------------------\n",
      "133: Train loss = 0.3037\n",
      "133: Validation loss = 0.3019\n",
      "------------------------------------------------\n",
      "134: Train loss = 0.3054\n",
      "134: Validation loss = 0.2909\n",
      "------------------------------------------------\n",
      "135: Train loss = 0.3042\n",
      "135: Validation loss = 0.2968\n",
      "------------------------------------------------\n",
      "136: Train loss = 0.2996\n",
      "136: Validation loss = 0.2943\n",
      "------------------------------------------------\n",
      "137: Train loss = 0.2913\n",
      "137: Validation loss = 0.2910\n",
      "------------------------------------------------\n",
      "138: Train loss = 0.2940\n",
      "138: Validation loss = 0.2948\n",
      "------------------------------------------------\n",
      "139: Train loss = 0.3052\n",
      "139: Validation loss = 0.2975\n",
      "------------------------------------------------\n",
      "140: Train loss = 0.2981\n",
      "140: Validation loss = 0.2951\n",
      "------------------------------------------------\n",
      "141: Train loss = 0.3039\n",
      "141: Validation loss = 0.2886\n",
      "------------------------------------------------\n",
      "142: Train loss = 0.2884\n",
      "142: Validation loss = 0.2921\n",
      "------------------------------------------------\n",
      "143: Train loss = 0.2885\n",
      "143: Validation loss = 0.2935\n",
      "------------------------------------------------\n",
      "144: Train loss = 0.2821\n",
      "144: Validation loss = 0.2923\n",
      "------------------------------------------------\n",
      "145: Train loss = 0.2867\n",
      "145: Validation loss = 0.2928\n",
      "------------------------------------------------\n",
      "146: Train loss = 0.2889\n",
      "146: Validation loss = 0.2960\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 15\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Train loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dl)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(total_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dl))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/adam.py:364\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    363\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 364\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    367\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in (range(200)):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for cell,mask,_ in (dl):\n",
    "        optimizer.zero_grad()\n",
    "        image = cell.to(device)\n",
    "        mask = (mask.float()).to(device)\n",
    "        out = model(image)\n",
    "        loss = criterion(out, mask)\n",
    "        total_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'{epoch}: Train loss = {total_loss/len(dl):.4f}')\n",
    "    train_losses.append(total_loss/len(dl))\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for cell,mask,_ in (val_dl):\n",
    "            image = cell.to(device)\n",
    "            mask = (mask.float()).to(device)\n",
    "            out = model(image)\n",
    "            val_loss = criterion(out, mask)\n",
    "            total_val_loss += val_loss\n",
    "        print(f'{epoch}: Validation loss = {total_val_loss/len(val_dl):.4f}')\n",
    "        val_losses.append(total_val_loss/len(val_dl))\n",
    "        \n",
    "    lr_scheduler.step()\n",
    "    if total_val_loss/len(val_dl) < best_val_loss:\n",
    "        best_val_loss = total_val_loss/len(val_dl)\n",
    "        print('Best val loss updated, model saved')\n",
    "        torch.save(model, f'/workspace/jay/DDP/Ocelot/cell_seg/upp_dice_ckpts/{epoch}_{total_val_loss/len(val_dl):.5f}.pt')\n",
    "    print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc2bf62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e85747d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd7560990a0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x.detach().cpu() for x in train_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73032dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd6ec780550>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x.detach().cpu() for x in val_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0241916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1d1601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
